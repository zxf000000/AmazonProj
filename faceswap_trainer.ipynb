{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faceswap-trainer-work.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxf000000/AmazonProj/blob/main/faceswap_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RROOaiIBkTHS"
      },
      "cell_type": "markdown",
      "source": [
        "# Faceswap Trainer Notebook\n",
        "\n",
        "**Description:**\n",
        "\n",
        "This is a jupyter notebook for training faceswap models on Google Colaboratory or colab. You will get a NVIDIA K80 GPU with 12GB of VRAM available for traning. It's advised that you use an external storage to store your models like Google Drive so you don't loose your work if something happens to the colab instance. You should have the traning image files on the colab machine to avoid network syncing. The instructions are based on my personal workflow but you can change where or how you save and load your data.\n",
        "\n",
        "**Instruction:**\n",
        "\n",
        "You should check that your instance has a GPU available.\n",
        "\n",
        "*   Go to top menu Runtime -> Change runtime type -> set Python3 and GPU\n",
        "*   When done run the GPU status notebook to check GPU information\n",
        "\n",
        "In the import packages section, uncomment the version of faceswap you would like to add. The first one is the latest faceswap code others are there for legacy reasons, version we know that work.\n",
        "\n",
        "Connect your Google Drive with the colab, this will mount your Google Drive to folder Drive, it will act as a local folder\n",
        "\n",
        "Alternativly you can upload images and models in zip files. Image set should be named set.zip and models models.zip. Run a cell and then browse the appropriate zip file.\n",
        "\n",
        "Run the first cell in the faceswap traning section, that would initialize the image preview python functions\n",
        "\n",
        "Run train compand with appopriate params:\n",
        "\n",
        "\n",
        "*   -A - face A folder\n",
        "*   -B - face B folder\n",
        "*   -m - model path\n",
        "*   -t - model type name, trainers: original, dfaker, dfl_h128, iae, unblanced, villain\n",
        "*   -bs - batch size, use an approprite batch size based on what are you trying to achive and the available GPU memory\n",
        "*   -s - save interavl, detemines how often does the model save and updates the preview window. Syncing with cloud storage can be slow with large models\n",
        "\n",
        "If you are an advanced user, you can view the tran config ini file by runing the cell under traning. Cell under that one will update the config file with the cell contents\n",
        "\n",
        "To end traning press the stop icon once, that will start the end process. Wait for the model to save after that the cell will stop executing\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "\n",
        "*    Colab instance will shutdown after 12hours\n",
        "*    If you get an error no GPU available try again later\n",
        "*    You can shutdown the browser when executing a script after about an hour without the browser the notebook will terminate\n",
        "*    If saving to Google Drive you have to monitor the amount of free space on the drive, every save cycle it will send prevous model to the trash. If the space fills up the last save is going to get corrupted. The training will break and a model from the trash will have be restored\n",
        "*    You can run only 1 cell at a time\n"
      ]
    },
    {
      "metadata": {
        "id": "rS2r3y8M_FZ8"
      },
      "cell_type": "markdown",
      "source": [
        "# Get GPU status\n"
      ]
    },
    {
      "metadata": {
        "id": "_UFW54uqbtgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9252d145-f176-4c55-9737-c34f724479b2"
      },
      "cell_type": "code",
      "source": [
        "#@title Display Virtual Machine system information, use this to check the assigned GPU\n",
        "\n",
        "def install_dependencies():\n",
        "  !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi;\n",
        "  !pip install gputil;\n",
        "  !pip install psutil;\n",
        "  !pip install humanize;%%capture\n",
        "\n",
        "\n",
        "def printm():\n",
        " GPUs = GPU.getGPUs()\n",
        "\n",
        " if len(GPUs) == 0:\n",
        "  print(\"No GPU available.\")\n",
        "  return\n",
        "\n",
        " gpu = GPUs[0]\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "from IPython.utils import io\n",
        "from google.colab import drive\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "  install_dependencies()\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "\n",
        "import GPUtil as GPU\n",
        "\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n",
            "Gen RAM Free: 87.8 GB  | Proc size: 106.9 MB\n",
            "GPU RAM Free: 40506MB | Used: 0MB | Util   0% | Total 40960MB\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jtmSwsskcv6o"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ]
    },
    {
      "metadata": {
        "id": "D5UP7Q0Xc0-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf60116-97ea-4c0d-c952-ea2bd1b27db3"
      },
      "cell_type": "code",
      "source": [
        "#@title Clone Faceswap github repository\n",
        "\n",
        "!rm -rf faceswap\n",
        "!git clone https://github.com/deepfakes/faceswap.git faceswap\n",
        "\n",
        "# !pip install tensorflow-gpu==1.15.0\n",
        "!pip install -r faceswap/requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'faceswap'...\n",
            "remote: Enumerating objects: 14682, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/32)\u001b[K\rremote: Counting objects:   6% (2/32)\u001b[K\rremote: Counting objects:   9% (3/32)\u001b[K\rremote: Counting objects:  12% (4/32)\u001b[K\rremote: Counting objects:  15% (5/32)\u001b[K\rremote: Counting objects:  18% (6/32)\u001b[K\rremote: Counting objects:  21% (7/32)\u001b[K\rremote: Counting objects:  25% (8/32)\u001b[K\rremote: Counting objects:  28% (9/32)\u001b[K\rremote: Counting objects:  31% (10/32)\u001b[K\rremote: Counting objects:  34% (11/32)\u001b[K\rremote: Counting objects:  37% (12/32)\u001b[K\rremote: Counting objects:  40% (13/32)\u001b[K\rremote: Counting objects:  43% (14/32)\u001b[K\rremote: Counting objects:  46% (15/32)\u001b[K\rremote: Counting objects:  50% (16/32)\u001b[K\rremote: Counting objects:  53% (17/32)\u001b[K\rremote: Counting objects:  56% (18/32)\u001b[K\rremote: Counting objects:  59% (19/32)\u001b[K\rremote: Counting objects:  62% (20/32)\u001b[K\rremote: Counting objects:  65% (21/32)\u001b[K\rremote: Counting objects:  68% (22/32)\u001b[K\rremote: Counting objects:  71% (23/32)\u001b[K\rremote: Counting objects:  75% (24/32)\u001b[K\rremote: Counting objects:  78% (25/32)\u001b[K\rremote: Counting objects:  81% (26/32)\u001b[K\rremote: Counting objects:  84% (27/32)\u001b[K\rremote: Counting objects:  87% (28/32)\u001b[K\rremote: Counting objects:  90% (29/32)\u001b[K\rremote: Counting objects:  93% (30/32)\u001b[K\rremote: Counting objects:  96% (31/32)\u001b[K\rremote: Counting objects: 100% (32/32)\u001b[K\rremote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 14682 (delta 19), reused 16 (delta 16), pack-reused 14650 (from 2)\u001b[K\n",
            "Receiving objects: 100% (14682/14682), 198.70 MiB | 43.63 MiB/s, done.\n",
            "Resolving deltas: 100% (10480/10480), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: './faceswap/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DbohDTynPjaP"
      },
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive folders\n",
        "---\n",
        "\n",
        "Run and paste the code account code\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KFvh38VHPoxl"
      },
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive only\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-nPdeX6cBUP2"
      },
      "cell_type": "markdown",
      "source": [
        "# Load local files upload\n"
      ]
    },
    {
      "metadata": {
        "id": "1S-bXDMFBZbT"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "local_download_path = os.path.expanduser('~/face1')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "!cd face1\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "#!bash -c 'mv *.zip /content/face1'\n",
        "!unzip set.zip -d face1\n",
        "!rm set.zip\n",
        "!ls face1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lq7c1Xq0LAyL"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "!rm -rf face2\n",
        "local_download_path = os.path.expanduser('~/face2')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "#!bash -c 'mv *.jpg /content/face2'\n",
        "\n",
        "!unzip set.zip -d face2\n",
        "!rm set.zip\n",
        "\n",
        "!ls face2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdEs482GymvT"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "!rm originalHighRes.zip\n",
        "local_download_path = os.path.expanduser('~/models')\n",
        "try:\n",
        "  os.makedirs(local_download_path)\n",
        "except: pass\n",
        "#!cd models\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "!unzip models.zip -d models\n",
        "!rm models.zip\n",
        "!ls models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48JERJbsTwS8"
      },
      "cell_type": "markdown",
      "source": [
        "# Faceswap Training\n"
      ]
    },
    {
      "metadata": {
        "id": "H5qMvYA7YHjV"
      },
      "cell_type": "code",
      "source": [
        "#Threading stuff stolen from:\n",
        "# https://stackoverflow.com/questions/32081926/a-new-thread-for-running-a-cell-in-ipython-jupyter-notebook\n",
        "\n",
        "from time import sleep,time\n",
        "from IPython.display import display,HTML\n",
        "import base64\n",
        "from threading import Thread\n",
        "\n",
        "# new faceswap version\n",
        "image_path = \"faceswap/training_preview.jpg\"\n",
        "\n",
        "# old faceswap\n",
        "#image_path = \"faceswap/_sample_training.jpg\"\n",
        "#image_path = \"faceswap/_sample_training using <OriginalHighRes: v=2.7, enc=ORIGINAL, encoder_dim=1024, img_shape=128x128>, bs=16.jpg\"\n",
        "!touch '{image_path}'\n",
        "\n",
        "class PreviewImg:\n",
        "  def __init__(self):\n",
        "    self.cancel = False\n",
        "    self.fig = display(HTML('<img src=\"https://dummyimage.com/1024x618/000/ffffff&text=Preview+Loading...\" />'), display_id=True)\n",
        "    print(\"Created display.\")\n",
        "\n",
        "  def update(self):\n",
        "    print(\"Updating display.\")\n",
        "    with open(image_path, 'rb') as image:\n",
        "      encoded = str(base64.b64encode(image.read()))[2:-1]\n",
        "      self.fig.update(HTML('<img src=\"data:image/jpg;base64,' + encoded + '\" width=\"1024\" height=\"618\" />'))\n",
        "      # print(\"Updated display.\")\n",
        "\n",
        "  def task(self):\n",
        "    while not self.cancel:\n",
        "      sleep(5 * 60) #Every 5 minutes.\n",
        "      if not self.fig:\n",
        "        print(\"Figure doesn't exist.\")\n",
        "        break\n",
        "      self.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHWQtYhvOMPE"
      },
      "cell_type": "code",
      "source": [
        "!echo \"Preview image size is reduced 3 times right click and save the image to view it in full size\"\n",
        "\n",
        "prev_img = PreviewImg()\n",
        "thread = Thread(target=prev_img.task)\n",
        "thread.start()\n",
        "\n",
        "!python3 faceswap/faceswap.py train -A 'face1' -B 'face2' -m '/content/drive/My Drive/models' -t 'villain' -bs 16 -s 330 -w -nl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoxsqJeCT0hT"
      },
      "cell_type": "code",
      "source": [
        "# uncomment if you have local models or files/commented are examples\n",
        "# first line display contents of the folder models orther lines are for downloads\n",
        "\n",
        "#!ls models\n",
        "#files.download('models/decoder_A.h5')\n",
        "#files.download('models/decoder_B.h5')\n",
        "#files.download('models/encoder.h5')\n",
        "#files.download('models/decoder_A.h5.bk')\n",
        "#files.download('models/decoder_B.h5.bk')\n",
        "#files.download('models/encoder.h5.bk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9WdKagHJ3m1y"
      },
      "cell_type": "code",
      "source": [
        "# Shows traning config file\n",
        "# Paste to cell under and run to reset the config\n",
        "!cat faceswap/config/train.ini"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2S1oZA2yhlaA"
      },
      "cell_type": "code",
      "source": [
        "# Run to update the model config file\n",
        "\n",
        "config = \"\"\"\n",
        "[global]\n",
        "# OPTIONS THAT APPLY TO ALL MODELS\n",
        "\n",
        "# Use ICNR Kernel Initializer for upscaling.\n",
        "# This can help reduce the 'checkerboard effect' when upscaling the image.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "icnr_init = False\n",
        "\n",
        "# Use subpixel upscaling rather than pixel shuffler.\n",
        "# Might increase speed at cost of VRAM\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "subpixel_upscaling = False\n",
        "\n",
        "# Use reflect padding rather than zero padding.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "reflect_padding = False\n",
        "\n",
        "# If using a mask, Use DSSIM loss for Mask training rather than Mean Absolute Error\n",
        "# May increase overall quality.\n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "dssim_mask_loss = True\n",
        "\n",
        "# If using a mask, Use Penalized loss for Mask training. Can stack with DSSIM.\n",
        "# May increase overall quality.\n",
        "# Choose from: True, False\n",
        "# [Default: True]\n",
        "penalized_mask_loss = True\n",
        "\n",
        "[model.dfaker]\n",
        "# DFAKER MODEL (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)\n",
        "\n",
        "# The mask to be used for training. Select none to not use a mask\n",
        "# Choose from: ['none', 'dfaker', 'dfl_full']\n",
        "# [Default: dfaker]\n",
        "mask_type = dfaker\n",
        "\n",
        "# How much of the extracted image to train on. Generally the model is optimized\n",
        "# to the default value. Sensible values to use are:\n",
        "# \t62.5%% spans from eyebrow to eyebrow.\n",
        "# \t75.0%% spans from temple to temple.\n",
        "# \t87.5%% spans from ear to ear.\n",
        "# \t100.0%% is a mugshot.\n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 100.0]\n",
        "coverage = 100.0\n",
        "\n",
        "[model.dfl_h128]\n",
        "# DFL H128 MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "# The mask to be used for training. Select none to not use a mask\n",
        "# Choose from: ['none', 'dfaker', 'dfl_full']\n",
        "# [Default: dfl_full]\n",
        "mask_type = dfl_full\n",
        "\n",
        "# How much of the extracted image to train on. Generally the model is optimized\n",
        "# to the default value. Sensible values to use are:\n",
        "# \t62.5%% spans from eyebrow to eyebrow.\n",
        "# \t75.0%% spans from temple to temple.\n",
        "# \t87.5%% spans from ear to ear.\n",
        "# \t100.0%% is a mugshot.\n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 62.5]\n",
        "coverage = 62.5\n",
        "\n",
        "[model.iae]\n",
        "# INTERMEDIATE AUTO ENCODER. BASED ON ORIGINAL MODEL, USES INTERMEDIATE LAYERS TO TRY TO BETTER GET DETAILS\n",
        "\n",
        "# Use DSSIM for Loss rather than Mean Absolute Error\n",
        "# May increase overall quality.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "dssim_loss = False\n",
        "\n",
        "# The mask to be used for training. Select none to not use a mask\n",
        "# Choose from: ['none', 'dfaker', 'dfl_full']\n",
        "# [Default: none]\n",
        "mask_type = none\n",
        "\n",
        "# How much of the extracted image to train on. Generally the model is optimized\n",
        "# to the default value. Sensible values to use are:\n",
        "# \t62.5%% spans from eyebrow to eyebrow.\n",
        "# \t75.0%% spans from temple to temple.\n",
        "# \t87.5%% spans from ear to ear.\n",
        "# \t100.0%% is a mugshot.\n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 62.5]\n",
        "coverage = 62.5\n",
        "\n",
        "[model.original]\n",
        "# ORIGINAL FACESWAP MODEL\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "# Use DSSIM for Loss rather than Mean Absolute Error\n",
        "# May increase overall quality.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "dssim_loss = False\n",
        "\n",
        "# The mask to be used for training. Select none to not use a mask\n",
        "# Choose from: ['none', 'dfaker', 'dfl_full']\n",
        "# [Default: none]\n",
        "mask_type = none\n",
        "\n",
        "# How much of the extracted image to train on. Generally the model is optimized\n",
        "# to the default value. Sensible values to use are:\n",
        "# \t62.5%% spans from eyebrow to eyebrow.\n",
        "# \t75.0%% spans from temple to temple.\n",
        "# \t87.5%% spans from ear to ear.\n",
        "# \t100.0%% is a mugshot.\n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 62.5]\n",
        "coverage = 62.5\n",
        "\n",
        "[model.unbalanced]\n",
        "# AN UNBALANCED MODEL WITH ADJUSTABLE INPUT SIZE OPTIONS.\n",
        "# THIS IS AN UNBALANCED MODEL SO B>A SWAPS MAY NOT WORK WELL\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other. NB: lowmem will override cutom nodes and complexity settings.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "# Use DSSIM for Loss rather than Mean Absolute Error\n",
        "# May increase overall quality.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "dssim_loss = False\n",
        "\n",
        "# The mask to be used for training. Select none to not use a mask\n",
        "# Choose from: ['none', 'dfaker', 'dfl_full']\n",
        "# [Default: none]\n",
        "mask_type = none\n",
        "\n",
        "# Number of nodes for decoder. Don't change this unless you know what you are doing!\n",
        "# Select an integer between 512 and 4096\n",
        "# [Default: 1024]\n",
        "nodes = 1024\n",
        "\n",
        "# Encoder Convolution Layer Complexity. sensible ranges: 128 to 160\n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 128]\n",
        "complexity_encoder = 128\n",
        "\n",
        "# Decoder A Complexity.\n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 384]\n",
        "complexity_decoder_a = 384\n",
        "\n",
        "# Decoder B Complexity.\n",
        "# Select an integer between 64 and 1024\n",
        "# [Default: 512]\n",
        "complexity_decoder_b = 512\n",
        "\n",
        "# Resolution (in pixels) of the image to train on.\n",
        "# BE AWARE Larger resolution will dramatically increaseVRAM requirements.\n",
        "# Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).\n",
        "# NB: Your faceset must be at least 1.6x larger than your required input size.\n",
        "#     (e.g. 160 is the maximum input size for a 256x256 faceset)\n",
        "# Select an integer between 64 and 512\n",
        "# [Default: 128]\n",
        "input_size = 128\n",
        "\n",
        "# How much of the extracted image to train on. Generally the model is optimized\n",
        "# to the default value. Sensible values to use are:\n",
        "# \t62.5%% spans from eyebrow to eyebrow.\n",
        "# \t75.0%% spans from temple to temple.\n",
        "# \t87.5%% spans from ear to ear.\n",
        "# \t100.0%% is a mugshot.\n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 62.5]\n",
        "coverage = 62.5\n",
        "\n",
        "[model.villain]\n",
        "# A HIGHER RESOLUTION VERSION OF THE ORIGINAL MODEL BY VILLAINGUY.\n",
        "# EXTREMELY VRAM HEAVY. FULL MODEL REQUIRES 9GB+ FOR BATCHSIZE 16\n",
        "\n",
        "# Lower memory mode. Set to 'True' if having issues with VRAM useage.\n",
        "# NB: Models with a changed lowmem mode are not compatible with each other.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "lowmem = False\n",
        "\n",
        "# Use DSSIM for Loss rather than Mean Absolute Error\n",
        "# May increase overall quality.\n",
        "# Choose from: True, False\n",
        "# [Default: False]\n",
        "dssim_loss = True\n",
        "\n",
        "# The mask to be used for training. Select none to not use a mask\n",
        "# Choose from: ['none', 'dfaker', 'dfl_full']\n",
        "# [Default: none]\n",
        "mask_type = none\n",
        "\n",
        "# How much of the extracted image to train on. Generally the model is optimized\n",
        "# to the default value. Sensible values to use are:\n",
        "# \t62.5%% spans from eyebrow to eyebrow.\n",
        "# \t75.0%% spans from temple to temple.\n",
        "# \t87.5%% spans from ear to ear.\n",
        "# \t100.0%% is a mugshot.\n",
        "# Select a decimal number between 62.5 and 100.0\n",
        "# [Default: 62.5]\n",
        "coverage = 75\n",
        "\"\"\"\n",
        "\n",
        "with open(\"faceswap/config/train.ini\", \"w\") as text_file:\n",
        "    text_file.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qsQoadKKHXSp"
      },
      "cell_type": "markdown",
      "source": [
        "# MachineTube Repository Model Export\n",
        "\n",
        "Models trained as Original can be exported to machinetube format. Models can be added to https://www.machine.tube/models\n",
        "Run the first cell to get the conversion code.\n",
        "Before runing the conversion cell set the model path\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "i1t9g5SDIq_K"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/transcranial/keras-js.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHmok2KNIrKJ"
      },
      "cell_type": "code",
      "source": [
        "# set the path to the model files\n",
        "model_path = 'Drive/models'\n",
        "\n",
        "import os\n",
        "mode_paths = [f for f in os.listdir(model_path) if os.path.splitext(f)[1][1:] == 'h5']\n",
        "\n",
        "print (mode_paths)\n",
        "\n",
        "for file in mode_paths:\n",
        "  full_path = os.path.join(model_path, file)\n",
        "  !python3 keras-js/python/encoder.py $full_path -q"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}